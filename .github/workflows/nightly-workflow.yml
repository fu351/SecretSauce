name: Nightly Workflow

on:
  schedule:
    # Runs daily at 11:55 UTC
    - cron: '55 11 * * *'
  workflow_dispatch:
    inputs:
      purge_mode:
        description: "execute or dry-run"
        required: false
        type: choice
        options:
          - execute
          - dry-run
        default: execute
      skip_purge:
        description: "Skip purge step (manual runs only)"
        required: false
        type: boolean
        default: false
      stores_csv:
        description: "Comma-separated stores (blank = all)"
        required: false
        type: string
        default: ""
      scraper_state:
        description: "2-letter state code filter (e.g., CA, AZ)"
        required: false
        type: string
        default: "CA"
      scraper_cities_csv:
        description: "Comma-separated Title Case cities (blank = all)"
        required: false
        type: string
        default: "Alameda,Albany,American Canyon,Antioch,Atherton,Belmont,Belvedere,Benicia,Berkeley,Brentwood,Brisbane,Burlingame,Calistoga,Campbell,Clayton,Cloverdale,Colma,Concord,Corte Madera,Cotati,Cupertino,Daly City,Danville,Dixon,Dublin,East Palo Alto,El Cerrito,Emeryville,Fairfax,Fairfield,Foster City,Fremont,Gilroy,Half Moon Bay,Hayward,Healdsburg,Hercules,Hillsborough,Lafayette,Larkspur,Livermore,Los Altos,Los Altos Hills,Los Gatos,Martinez,Menlo Park,Mill Valley,Millbrae,Milpitas,Monte Sereno,Moraga,Morgan Hill,Mountain View,Napa,Newark,Novato,Oakland,Oakley,Orinda,Pacifica,Palo Alto,Petaluma,Piedmont,Pinole,Pittsburg,Pleasant Hill,Pleasanton,Portola Valley,Redwood City,Richmond,Rio Vista,Rohnert Park,Ross,San Anselmo,San Bruno,San Carlos,San Francisco,San Jose,San Leandro,San Mateo,San Pablo,San Rafael,San Ramon,Santa Clara,Santa Rosa,Saratoga,Sausalito,Sebastopol,Sonoma,South San Francisco,Suisun City,Sunnyvale,Tiburon,Union City,Vacaville,Vallejo,Walnut Creek,Windsor,Woodside,Yountville,St. Helena,Saint Helena"
      scraper_zip_min:
        description: "Minimum ZIP filter (blank = no lower bound)"
        required: false
        type: string
        default: ""
      scraper_zip_max:
        description: "Maximum ZIP filter (blank = no upper bound)"
        required: false
        type: string
        default: ""
      ingredient_limit:
        description: "Limit ingredients scraped (0 = all)"
        required: false
        type: string
        default: "0"
      store_limit:
        description: "Limit store rows queried (0 = all)"
        required: false
        type: string
        default: "0"
      store_concurrency:
        description: "Concurrent store scrapes per ingredient"
        required: false
        type: string
        default: "20"
      ingredient_delay_ms:
        description: "Delay between ingredients in milliseconds"
        required: false
        type: string
        default: "1000"
      insert_batch_size:
        description: "RPC insert batch size"
        required: false
        type: string
        default: "500"
      min_expected_rows:
        description: "Fail if below this row count"
        required: false
        type: string
        default: "100"
      scraper_batch_size:
        description: "Default ingredient chunk size for all stores"
        required: false
        type: string
        default: "20"
      scraper_batch_concurrency:
        description: "Default batch worker concurrency for all stores"
        required: false
        type: string
        default: "4"
      scraper_timeout_minutes:
        description: "Hard timeout per store job (minutes)"
        required: false
        type: string
        default: "120"
      traderjoes_batch_size:
        description: "Trader Joe's ingredient chunk size"
        required: false
        type: string
        default: "20"
      traderjoes_batch_concurrency:
        description: "Trader Joe's batch worker concurrency"
        required: false
        type: string
        default: "2"
      queue_batch_limit:
        description: "Queue batch size"
        required: false
        type: string
        default: "100"
      queue_context:
        description: "Queue context"
        required: false
        type: choice
        options:
          - pantry
          - recipe
        default: pantry
      queue_resolver_name:
        description: "Queue resolver run name"
        required: false
        type: string
        default: "nightly-openai"
      queue_dry_run:
        description: "Dry run queue stage"
        required: false
        type: choice
        options:
          - "false"
          - "true"
        default: "false"
      queue_max_batches:
        description: "Maximum queue batch cycles"
        required: false
        type: string
        default: "10"
      queue_batch_delay_seconds:
        description: "Delay between queue batches in seconds"
        required: false
        type: string
        default: "2"

jobs:
  purge:
    name: Daily Purge
    if: ${{ github.event_name != 'workflow_dispatch' || !inputs.skip_purge }}
    uses: ./.github/workflows/daily-purge.yml
    secrets: inherit
    with:
      purge_mode: ${{ github.event.inputs.purge_mode || 'execute' }}

  seed-canonical-placeholder:
    name: Seed Canonical Placeholder
    needs: purge
    if: ${{ always() && (needs.purge.result == 'success' || needs.purge.result == 'skipped') }}
    runs-on: ubuntu-latest
    outputs:
      seeded: ${{ steps.seed.outputs.seeded }}
      placeholder_name: ${{ steps.seed.outputs.placeholder_name }}
    steps:
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install Supabase client (isolated)
        run: |
          mkdir -p /tmp/supabase-client
          npm install --no-save --no-package-lock --legacy-peer-deps --prefix /tmp/supabase-client @supabase/supabase-js

      - name: Seed placeholder canonical ingredient if empty
        id: seed
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          PLACEHOLDER_NAME: __nightly_workflow_placeholder_${{ github.run_id }}
        run: |
          node - <<'NODE'
          const { createClient } = require('/tmp/supabase-client/node_modules/@supabase/supabase-js')
          const fs = require('fs')

          const supabaseUrl = process.env.SUPABASE_URL
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY
          const placeholderName = process.env.PLACEHOLDER_NAME
          const githubOutput = process.env.GITHUB_OUTPUT

          if (!supabaseUrl || !supabaseKey) {
            console.error('‚ùå Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY')
            process.exit(1)
          }

          if (!placeholderName) {
            console.error('‚ùå Missing PLACEHOLDER_NAME')
            process.exit(1)
          }

          const supabase = createClient(supabaseUrl, supabaseKey)

          async function run() {
            const { count, error: countError } = await supabase
              .from('standardized_ingredients')
              .select('id', { head: true, count: 'exact' })

            if (countError) {
              console.error('‚ùå Failed counting standardized_ingredients rows:', countError)
              process.exit(1)
            }

            if ((count ?? 0) > 0) {
              console.log(`‚úÖ standardized_ingredients has ${count} row(s); no placeholder needed`)
              fs.appendFileSync(githubOutput, 'seeded=false\n')
              fs.appendFileSync(githubOutput, 'placeholder_name=\n')
              return
            }

            const { error: insertError } = await supabase
              .from('standardized_ingredients')
              .insert({ canonical_name: placeholderName, category: 'other' })

            if (insertError) {
              console.error('‚ùå Failed inserting placeholder canonical ingredient:', insertError)
              process.exit(1)
            }

            console.log(`üß© Inserted placeholder canonical ingredient: ${placeholderName}`)
            fs.appendFileSync(githubOutput, 'seeded=true\n')
            fs.appendFileSync(githubOutput, `placeholder_name=${placeholderName}\n`)
          }

          run().catch(error => {
            console.error('‚ùå Unexpected seed error:', error)
            process.exit(1)
          })
          NODE

  scraper:
    name: Daily Scraper Matrix
    needs: seed-canonical-placeholder
    if: ${{ always() && needs['seed-canonical-placeholder'].result == 'success' }}
    uses: ./.github/workflows/daily-scraper-matrix.yml
    secrets: inherit
    with:
      stores_csv: ${{ github.event.inputs.stores_csv || '' }}
      scraper_state: ${{ github.event.inputs.scraper_state || 'CA' }}
      scraper_cities_csv: ${{ github.event.inputs.scraper_cities_csv || 'Alameda,Albany,American Canyon,Antioch,Atherton,Belmont,Belvedere,Benicia,Berkeley,Brentwood,Brisbane,Burlingame,Calistoga,Campbell,Clayton,Cloverdale,Colma,Concord,Corte Madera,Cotati,Cupertino,Daly City,Danville,Dixon,Dublin,East Palo Alto,El Cerrito,Emeryville,Fairfax,Fairfield,Foster City,Fremont,Gilroy,Half Moon Bay,Hayward,Healdsburg,Hercules,Hillsborough,Lafayette,Larkspur,Livermore,Los Altos,Los Altos Hills,Los Gatos,Martinez,Menlo Park,Mill Valley,Millbrae,Milpitas,Monte Sereno,Moraga,Morgan Hill,Mountain View,Napa,Newark,Novato,Oakland,Oakley,Orinda,Pacifica,Palo Alto,Petaluma,Piedmont,Pinole,Pittsburg,Pleasant Hill,Pleasanton,Portola Valley,Redwood City,Richmond,Rio Vista,Rohnert Park,Ross,San Anselmo,San Bruno,San Carlos,San Francisco,San Jose,San Leandro,San Mateo,San Pablo,San Rafael,San Ramon,Santa Clara,Santa Rosa,Saratoga,Sausalito,Sebastopol,Sonoma,South San Francisco,Suisun City,Sunnyvale,Tiburon,Union City,Vacaville,Vallejo,Walnut Creek,Windsor,Woodside,Yountville,St. Helena,Saint Helena' }}
      scraper_zip_min: ${{ github.event.inputs.scraper_zip_min || '' }}
      scraper_zip_max: ${{ github.event.inputs.scraper_zip_max || '' }}
      ingredient_limit: ${{ github.event.inputs.ingredient_limit || '0' }}
      store_limit: ${{ github.event.inputs.store_limit || '0' }}
      store_concurrency: ${{ github.event.inputs.store_concurrency || '20' }}
      ingredient_delay_ms: ${{ github.event.inputs.ingredient_delay_ms || '1000' }}
      insert_batch_size: ${{ github.event.inputs.insert_batch_size || '500' }}
      min_expected_rows: ${{ github.event.inputs.min_expected_rows || '100' }}
      scraper_batch_size: ${{ github.event.inputs.scraper_batch_size || '20' }}
      scraper_batch_concurrency: ${{ github.event.inputs.scraper_batch_concurrency || '4' }}
      scraper_timeout_minutes: ${{ github.event.inputs.scraper_timeout_minutes || '44' }}
      traderjoes_batch_size: ${{ github.event.inputs.traderjoes_batch_size || '20' }}
      traderjoes_batch_concurrency: ${{ github.event.inputs.traderjoes_batch_concurrency || '2' }}

  update-unit-estimates:
    name: Update Unit Weight Estimates
    needs: scraper
    uses: ./.github/workflows/update-unit-weight-estimates.yml
    secrets: inherit

  queue:
    name: Nightly Ingredient Queue
    needs: update-unit-estimates
    if: ${{ always() }}
    uses: ./.github/workflows/nightly-ingredient-queue.yml
    secrets: inherit
    with:
      queue_batch_limit: ${{ github.event.inputs.queue_batch_limit || '100' }}
      queue_context: ${{ github.event.inputs.queue_context || 'pantry' }}
      queue_resolver_name: ${{ github.event.inputs.queue_resolver_name || 'nightly-openai' }}
      queue_dry_run: ${{ github.event.inputs.queue_dry_run || 'false' }}
      openai_model: ${{ github.event.inputs.openai_model || 'gpt-4o-mini' }}
      gemini_model: ${{ github.event.inputs.gemini_model || 'gemini-3-flash-preview' }}
      queue_max_batches: ${{ github.event.inputs.queue_max_batches || '10' }}
      queue_batch_delay_seconds: ${{ github.event.inputs.queue_batch_delay_seconds || '2' }}

  cleanup-canonical-placeholder:
    name: Cleanup Canonical Placeholder
    needs:
      - seed-canonical-placeholder
      - queue
    if: ${{ always() && needs['seed-canonical-placeholder'].result == 'success' }}
    runs-on: ubuntu-latest
    steps:
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install Supabase client (isolated)
        run: |
          mkdir -p /tmp/supabase-client
          npm install --no-save --no-package-lock --legacy-peer-deps --prefix /tmp/supabase-client @supabase/supabase-js

      - name: Delete placeholder canonical ingredient
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SEEDED: ${{ needs['seed-canonical-placeholder'].outputs.seeded }}
          PLACEHOLDER_NAME: ${{ needs['seed-canonical-placeholder'].outputs.placeholder_name }}
        run: |
          node - <<'NODE'
          const { createClient } = require('/tmp/supabase-client/node_modules/@supabase/supabase-js')

          const supabaseUrl = process.env.SUPABASE_URL
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY
          const seeded = String(process.env.SEEDED || '').toLowerCase() === 'true'
          const placeholderName = process.env.PLACEHOLDER_NAME

          if (!seeded || !placeholderName) {
            console.log('‚úÖ No placeholder inserted in this run; skipping cleanup')
            process.exit(0)
          }

          if (!supabaseUrl || !supabaseKey) {
            console.error('‚ùå Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY')
            process.exit(1)
          }

          const supabase = createClient(supabaseUrl, supabaseKey)

          async function run() {
            const { error } = await supabase
              .from('standardized_ingredients')
              .delete()
              .eq('canonical_name', placeholderName)

            if (error) {
              console.error('‚ùå Failed deleting placeholder canonical ingredient:', error)
              process.exit(1)
            }

            console.log(`üßπ Deleted placeholder canonical ingredient: ${placeholderName}`)
          }

          run().catch(error => {
            console.error('‚ùå Unexpected cleanup error:', error)
            process.exit(1)
          })
          NODE
