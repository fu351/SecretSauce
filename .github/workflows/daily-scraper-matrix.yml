name: Daily Scraper Matrix

on:
  workflow_call:
    inputs:
      stores_csv:
        description: "Comma-separated stores (blank = all)"
        required: false
        type: string
        default: ""
      ingredient_limit:
        description: "Limit ingredients scraped (0 = all)"
        required: false
        type: string
        default: "0"
      store_limit:
        description: "Limit store rows queried (0 = all)"
        required: false
        type: string
        default: "0"
      store_concurrency:
        description: "Concurrent store scrapes per ingredient"
        required: false
        type: string
        default: "20"
      ingredient_delay_ms:
        description: "Delay between ingredients in milliseconds"
        required: false
        type: string
        default: "1000"
      insert_batch_size:
        description: "RPC insert batch size"
        required: false
        type: string
        default: "500"
      min_expected_rows:
        description: "Fail verify step if below this row count"
        required: false
        type: string
        default: "100"
      request_timeout_ms:
        description: "Per-request scraper timeout in milliseconds"
        required: false
        type: string
        default: "20000"
      scraper_timeout_minutes:
        description: "Hard timeout per store job (minutes)"
        required: false
        type: string
        default: "40"
      scraper_batch_size:
        description: "Default ingredient chunk size for all stores"
        required: false
        type: string
        default: "20"
      scraper_batch_concurrency:
        description: "Default batch worker concurrency for all stores"
        required: false
        type: string
        default: "4"
      traderjoes_batch_size:
        description: "Trader Joe's ingredient chunk size"
        required: false
        type: string
        default: "20"
      traderjoes_batch_concurrency:
        description: "Trader Joe's batch worker concurrency"
        required: false
        type: string
        default: "2"
  workflow_dispatch:
    inputs:
      stores_csv:
        description: "Comma-separated stores (blank = all)"
        required: false
        type: string
        default: ""
      ingredient_limit:
        description: "Limit ingredients scraped (0 = all)"
        required: false
        type: string
        default: "0"
      store_limit:
        description: "Limit store rows queried (0 = all)"
        required: false
        type: string
        default: "0"
      store_concurrency:
        description: "Concurrent store scrapes per ingredient"
        required: false
        type: string
        default: "20"
      ingredient_delay_ms:
        description: "Delay between ingredients in milliseconds"
        required: false
        type: string
        default: "1000"
      insert_batch_size:
        description: "RPC insert batch size"
        required: false
        type: string
        default: "500"
      min_expected_rows:
        description: "Fail verify step if below this row count"
        required: false
        type: string
        default: "100"
      request_timeout_ms:
        description: "Per-request scraper timeout in milliseconds"
        required: false
        type: string
        default: "20000"
      scraper_timeout_minutes:
        description: "Hard timeout per store job (minutes)"
        required: false
        type: string
        default: "40"
      scraper_batch_size:
        description: "Default ingredient chunk size for all stores"
        required: false
        type: string
        default: "20"
      scraper_batch_concurrency:
        description: "Default batch worker concurrency for all stores"
        required: false
        type: string
        default: "4"
      traderjoes_batch_size:
        description: "Trader Joe's ingredient chunk size"
        required: false
        type: string
        default: "20"
      traderjoes_batch_concurrency:
        description: "Trader Joe's batch worker concurrency"
        required: false
        type: string
        default: "2"

concurrency:
  # Dedupes overlapping runs for the same ref+store selection while allowing queued execution.
  group: daily-scraper-matrix-${{ github.ref }}-${{ inputs.stores_csv || 'all' }}
  cancel-in-progress: false

jobs:
  prepare-matrix:
    name: Prepare Store Matrix
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      store_json: ${{ steps.build.outputs.store_json }}

    steps:
      - name: Build store matrix
        id: build
        env:
          STORES_CSV: ${{ inputs.stores_csv }}
        run: |
          set -euo pipefail

          # Temporarily disable Safeway from nightly matrix runs.
          # Whole Foods temporarily disabled.
          default_stores=(walmart target kroger aldi traderjoes andronicos meijer 99ranch)

          if [ -z "${STORES_CSV// }" ]; then
            selected=("${default_stores[@]}")
          else
            normalized=$(echo "$STORES_CSV" | tr '[:upper:]' '[:lower:]' | tr ';' ',' | tr -d ' ')
            IFS=',' read -r -a requested <<< "$normalized"
            selected=()
            declare -A seen=()
            duplicate_count=0

            for candidate in "${requested[@]}"; do
              [ -z "$candidate" ] && continue
              case "$candidate" in
                walmart|target|kroger|aldi|traderjoes|andronicos|meijer|99ranch)
                  if [ -z "${seen[$candidate]+x}" ]; then
                    selected+=("$candidate")
                    seen[$candidate]=1
                  else
                    duplicate_count=$((duplicate_count + 1))
                  fi
                  ;;
                *)
                  echo "‚ùå Unsupported store in stores_csv: $candidate"
                  echo "Allowed stores: ${default_stores[*]}"
                  exit 1
                  ;;
              esac
            done

            if [ "${#selected[@]}" -eq 0 ]; then
              echo "‚ùå No valid stores found in stores_csv"
              exit 1
            fi

            if [ "$duplicate_count" -gt 0 ]; then
              echo "‚ö†Ô∏è Deduped $duplicate_count duplicate store entries from stores_csv"
            fi
          fi

          json="["
          for i in "${!selected[@]}"; do
            [ "$i" -gt 0 ] && json+=","
            json+="\"${selected[$i]}\""
          done
          json+="]"

          echo "store_json=$json" >> "$GITHUB_OUTPUT"
          echo "Using stores: $json"

  scrape-stores:
    name: Scrape ${{ matrix.store }}
    needs: prepare-matrix
    runs-on: ubuntu-latest
    timeout-minutes: 60

    strategy:
      fail-fast: false
      max-parallel: 4
      matrix:
        store: ${{ fromJson(needs.prepare-matrix.outputs.store_json) }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: scripts/package-lock.json

      - name: Install dependencies
        timeout-minutes: 15
        run: npm install --legacy-peer-deps

      - name: Scrape ${{ matrix.store }}
        timeout-minutes: 42
        env:
          STORE_BRAND: ${{ matrix.store }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          INGREDIENT_LIMIT: ${{ inputs.ingredient_limit }}
          STORE_LIMIT: ${{ inputs.store_limit }}
          STORE_CONCURRENCY: ${{ inputs.store_concurrency }}
          INGREDIENT_DELAY_MS: ${{ inputs.ingredient_delay_ms }}
          INSERT_BATCH_SIZE: ${{ inputs.insert_batch_size }}
          SCRAPER_TIMEOUT_MS: ${{ inputs.request_timeout_ms }}
          SCRAPER_TIMEOUT_MINUTES: ${{ inputs.scraper_timeout_minutes }}
          SCRAPER_BATCH_SIZE: ${{ inputs.scraper_batch_size }}
          SCRAPER_BATCH_CONCURRENCY: ${{ inputs.scraper_batch_concurrency }}
          TRADERJOES_BATCH_SIZE: ${{ inputs.traderjoes_batch_size }}
          TRADERJOES_BATCH_CONCURRENCY: ${{ inputs.traderjoes_batch_concurrency }}
        run: |
          set -euo pipefail

          # Clamp configurable timeouts to keep runs safe from unbounded values.
          if ! [[ "${SCRAPER_TIMEOUT_MS:-}" =~ ^[0-9]+$ ]]; then SCRAPER_TIMEOUT_MS=20000; fi
          if [ "$SCRAPER_TIMEOUT_MS" -lt 5000 ]; then SCRAPER_TIMEOUT_MS=5000; fi
          if [ "$SCRAPER_TIMEOUT_MS" -gt 60000 ]; then SCRAPER_TIMEOUT_MS=60000; fi

          if ! [[ "${SCRAPER_TIMEOUT_MINUTES:-}" =~ ^[0-9]+$ ]]; then SCRAPER_TIMEOUT_MINUTES=40; fi
          if [ "$SCRAPER_TIMEOUT_MINUTES" -lt 10 ]; then SCRAPER_TIMEOUT_MINUTES=10; fi
          if [ "$SCRAPER_TIMEOUT_MINUTES" -gt 44 ]; then SCRAPER_TIMEOUT_MINUTES=44; fi

          if ! [[ "${SCRAPER_BATCH_SIZE:-}" =~ ^[0-9]+$ ]]; then SCRAPER_BATCH_SIZE=20; fi
          if [ "$SCRAPER_BATCH_SIZE" -lt 1 ]; then SCRAPER_BATCH_SIZE=1; fi
          if [ "$SCRAPER_BATCH_SIZE" -gt 200 ]; then SCRAPER_BATCH_SIZE=200; fi

          if ! [[ "${SCRAPER_BATCH_CONCURRENCY:-}" =~ ^[0-9]+$ ]]; then SCRAPER_BATCH_CONCURRENCY=4; fi
          if [ "$SCRAPER_BATCH_CONCURRENCY" -lt 1 ]; then SCRAPER_BATCH_CONCURRENCY=1; fi
          if [ "$SCRAPER_BATCH_CONCURRENCY" -gt 20 ]; then SCRAPER_BATCH_CONCURRENCY=20; fi

          if ! [[ "${TRADERJOES_BATCH_SIZE:-}" =~ ^[0-9]+$ ]]; then TRADERJOES_BATCH_SIZE=20; fi
          if [ "$TRADERJOES_BATCH_SIZE" -lt 1 ]; then TRADERJOES_BATCH_SIZE=1; fi
          if [ "$TRADERJOES_BATCH_SIZE" -gt 200 ]; then TRADERJOES_BATCH_SIZE=200; fi

          if ! [[ "${TRADERJOES_BATCH_CONCURRENCY:-}" =~ ^[0-9]+$ ]]; then TRADERJOES_BATCH_CONCURRENCY=2; fi
          if [ "$TRADERJOES_BATCH_CONCURRENCY" -lt 1 ]; then TRADERJOES_BATCH_CONCURRENCY=1; fi
          if [ "$TRADERJOES_BATCH_CONCURRENCY" -gt 10 ]; then TRADERJOES_BATCH_CONCURRENCY=10; fi

          export SCRAPER_TIMEOUT_MS
          export SCRAPER_BATCH_SIZE
          export SCRAPER_BATCH_CONCURRENCY
          export TRADERJOES_BATCH_SIZE
          export TRADERJOES_BATCH_CONCURRENCY

          echo "üöÄ Starting scraper for ${{ matrix.store }}"
          echo "   Request timeout (ms): $SCRAPER_TIMEOUT_MS"
          echo "   Job hard timeout (m): $SCRAPER_TIMEOUT_MINUTES"
          echo "   Default batch size: $SCRAPER_BATCH_SIZE"
          echo "   Default batch concurrency: $SCRAPER_BATCH_CONCURRENCY"
          echo "   Trader Joe's batch size: $TRADERJOES_BATCH_SIZE"
          echo "   Trader Joe's batch concurrency: $TRADERJOES_BATCH_CONCURRENCY"

          set +e
          timeout --signal=SIGTERM --kill-after=90s "${SCRAPER_TIMEOUT_MINUTES}m" node scripts/daily-scraper.js
          status=$?
          set -e

          if [ "$status" -eq 124 ]; then
            echo "‚ùå Scraper timed out after ${SCRAPER_TIMEOUT_MINUTES} minutes for ${{ matrix.store }}"
          fi

          exit "$status"
